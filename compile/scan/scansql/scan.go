package scansql

import (
	"fmt"
	"strings"

	"github.com/fioncat/go-gendb/build"
	"github.com/fioncat/go-gendb/compile/token"
	"github.com/fioncat/go-gendb/misc/errors"
	"github.com/fioncat/go-gendb/misc/iter"
	"github.com/fioncat/go-gendb/misc/set"
)

// Result is the result of scanning the sql file.
// Which contains multiple named SQL statements.
// The Result contains the names of all sql statements
// and their token slices. The token slice of the SQL
// statement needs to be further parsed later.
type Result struct {
	// The source file path
	Path string `json:"path"`

	// All sql statements
	Statements []Statement `json:"statements"`

	nameSet *set.Set
}

// Statement represents a SQL statement after lexical
// analysis, which includes the original SQL statement
// and token slices generated by lexical analysis. The
// sql statement here is named. It uses "--! {Name}" to
// mark the name in the sql file. "{name}" cannot be
// empty and will be stored in the Name field.
type Statement struct {
	// The source file path
	Path string `json:"path"`
	// line number of the sql statement
	LineNum int `json:"line_num"`

	// name of the sql
	Name string `json:"name"`

	// tokens
	Tokens   []token.Token `json:"-"`
	TokenStr string        `json:"tokens"`

	// origin sql string
	Origin string `json:"origin"`
}

// Do performs a scan on a sql file, scans all sql
// statements in it, and performs a lexical analysis on the
// sql statement to generate a token return. Each sql
// statement needs to be marked and separated by "--! {name}".
// The returned Result structure can be passed to the parser
// for grammatical analysis.
func Do(sourcePath, content string) (*Result, error) {
	return file(sourcePath, content)
}

func file(path, content string) (*Result, error) {
	var r Result
	r.Path = path

	r.nameSet = set.New()
	iter := iter.New(strings.Split(content, "\n"))

	var line string
	for {
		idx := iter.NextP(&line)
		if idx < 0 {
			break
		}
		line = strings.TrimSpace(line)
		if len(line) == 0 {
			continue
		}
		lineNum := idx + 1
		if token.SQL_TAG.Prefix(line) {
			name := strings.TrimLeft(line, token.SQL_TAG.Get())
			name = strings.TrimSpace(name)
			if name == "" {
				// we donot allow empty sql name
				return nil, errors.NewComp(path, lineNum,
					"sql name is empty")
			}
			if r.nameSet.Contains(name) {
				// name is duplicate
				return nil, errors.NewComp(path, lineNum,
					`sql name is duplcate for "%s"`, name)
			}
			r.nameSet.Append(name)

			var sm Statement
			sm.Path = path
			sm.LineNum = lineNum
			sm.Name = name

			var sqlLines []string
			for {
				pick := iter.Pick()
				if pick == nil {
					break
				}
				pickLine := pick.(string)
				if token.SQL_TAG.Prefix(pickLine) {
					break
				}
				sqlIdx := iter.NextP(&pickLine)
				if sqlIdx < 0 {
					break
				}
				if pickLine == "" ||
					token.SQL_COMMENT.Prefix(pickLine) {
					// ignore empty and comment line
					continue
				}
				sqlLines = append(sqlLines, pickLine)
			}

			sm.Origin = strings.Join(sqlLines, " ")

			var err error
			sm.Tokens, err = sql(path, lineNum, sm.Origin)
			if err != nil {
				return nil, err
			}

			if build.DEBUG {
				ts := fmt.Sprint(sm.Tokens)
				if len(ts) > 2 {
					ts = ts[1 : len(ts)-1]
					sm.TokenStr = ts
				}
			}

			r.Statements = append(r.Statements, sm)
		}
	}
	return &r, nil
}

const (
	minSQL_len = 7
)

func sql(path string, lineNum int, s string) ([]token.Token, error) {
	scanErr := func(chNum int, msg string, vs ...interface{}) ([]token.Token, error) {
		return nil, errors.NewComp(path, lineNum, msg, vs...).WithCharNum(chNum)
	}

	if len(s) <= minSQL_len {
		return scanErr(-1, "sql length is too small")
	}

	master := s[:minSQL_len-1]
	master = strings.ToUpper(master)
	switch master {
	case token.SQL_UPDATE.Get(), token.SQL_DELETE.Get(),
		token.SQL_INSERT.Get():
		// The modification statement has only a single
		// token that identifies the modification type
		return []token.Token{token.Key(master)}, nil
	case token.SQL_SELECT.Get():
		// ignore

	default:
		return scanErr(0, `unknown start of the sql: "%s"`, master)
	}

	bucket := token.NewBucket()
	bucket.SetKeywords(token.SQL_Keywords)
	iter := iter.New([]rune(s))

	var ch rune
	for {
		idx := iter.NextP(&ch)
		if idx < 0 {
			bucket.Indent()
			break
		}

		if token.SPACE.EqRune(ch) ||
			token.TABLE.EqRune(ch) ||
			token.BREAK.EqRune(ch) {
			// space, \t, \n
			bucket.Indent()
			continue
		}

		switch ch {
		case token.LPAREN.Rune():
			bucket.Key(token.LPAREN)

		case token.RPAREN.Rune():
			bucket.Key(token.RPAREN)

		case token.PERIOD.Rune():
			bucket.Key(token.PERIOD)

		case token.COMMA.Rune():
			bucket.Key(token.COMMA)

		case token.SQUO.Rune():
			fallthrough
		case token.QUO.Rune():
			bucket.Indent()
			var quotes []rune
			var subch rune
			for iter.Next(&subch) {
				if token.SQUO.EqRune(subch) ||
					token.QUO.EqRune(subch) {
					// next ' "
					break
				}
				quotes = append(quotes, subch)
			}
			bucket.AddToken(token.Indent(string(quotes)))

		default:
			bucket.Append(ch)
		}
	}
	bucket.Indent()
	return bucket.Get(), nil
}
